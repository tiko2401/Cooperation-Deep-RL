{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of implementation\n",
    "\n",
    "The solution is implemented in PyTorch with Python 3. It is a DDPG Actor Critic implementation to solve the Tennis environment from UnityEnvironment. The implementation trains two agents with individual actor and critic networks to play tennis over a net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descirption of algorithm\n",
    "\n",
    "The implementation contains an Actor Citic model where the actor is a fully connected linear model (_Actor_-class) with two hidden layers (256 and 128 units, respectively).The hidden layers are connected via a ReLu activation function while the final output layer uses tanh to yield an action-space sized output. The input layer receives the current state of both agents and thus has 48 units.\n",
    "The critic model (_Critic_-class) similarly consists of two hidden layers with 256 and 128 units. The main difference is the concatination with the action vector, the output size (1) and the use of batch normalization on the first layer only.\n",
    "\n",
    "For learning, the agents use the model in combination with Experience Replay which is initialized in the _Replay Buffer_ class. The _Update Every_ parameter controls after how many steps the model learns.\n",
    "\n",
    "Also, a parameter Epsilon was added that is used to reduce the noise the longer the agent trains and is designed to decay over time\n",
    "\n",
    "The hyperparameters were chosen as follows:\n",
    "- Buffer size for the Replay Buffer: 500000\n",
    "- Batch size: 64\n",
    "- Discount factor GAMMA: 0.98\n",
    "- Parameter for soft update (TAU): 0.005\n",
    "- Learning rate actor: 0.001\n",
    "- Learning rate critic: 0.001\n",
    "- Weight decay: 0\n",
    "- Update parameter to determine after how many steps the model learns: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of results\n",
    "\n",
    "<img src=\\\"Plot of results_solution.png\\\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of last 100 episodes\n",
    "\n",
    "<img src=\\\"Plot of last 100 episodes.png\\\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodes needed\n",
    "\n",
    "The total number of episodes that was needed to solve the environment with an average score of +0.5 over 100 episodes was about 1000 in the submitted implementation. However, while we aimed for a score of +0.9 we noticed that like in the benchmark implementation, the scores started to drop at some point and only reached the average score of +0.9 after 1374 episodes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ideas for future work\n",
    "\n",
    "Developing the model to work in combination with Prioritized Experience Replay or Hindsight Experience Replay seems interesting as well as comparing a PPO approach to the DDPG one. However, one of the most interesting topics would be to stabilize the training in order to not letting the scores drop at some point. Suggestions on this would be very welcome.\n",
    "Also, it would be interesting to see how the trained agent would perform in similar but different environments to see if (1) it can learn faster in a new environment than a completely new agent and (2) there are other parallels between different environments and problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
