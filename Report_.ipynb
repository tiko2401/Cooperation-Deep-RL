{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of implementation\n",
    "\n",
    "The solution is implemented in PyTorch with Python 3. It is a DDPG Actor Critic implementation to solve the Tennis environment from UnityEnvironment. The implementation trains two agents with individual actor and critic networks to play tennis over a net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of algorithm\n",
    "\n",
    "The implementation contains an Actor Citic model where the actor is a fully connected linear model (_Actor_-class) with two hidden layers (256 and 128 units, respectively). A DDPG actor critic implementation was used for two reasons: (1) Due to the hint in the Benchmark Implementation description in the class and (2) since I was able to re-use the code from the previous project (Continuous Control) and adjust it in proficient ways.\n",
    "The hidden layers are connected via a ReLu activation function while the final output layer uses tanh to yield an action-space sized output. The input layer receives the current state of both agents and thus has 48 units. The tanh activation layer is used due to the nature of the problem, where we want an output between -1 and 1 for the respective action vectors.\n",
    "\n",
    "The critic model (_Critic_-class) similarly consists of two hidden layers with 256 and 128 units. The main difference is the concatination with the action vector and the output size (1). The size of the model units was chosen based on the task: Since we have a continuous problem and take the observations of both agents as input, the first layer was designed to be larger than the second one to contain more weights and increase model accuracy. \n",
    "\n",
    "For learning, the agents use the model in combination with Experience Replay which is initialized in the _Replay Buffer_ class. Each agent has it's own Replay Buffer to sample from with a size of 500.000 in order to have a large number of observations we can sample from due to the continuous problem the agents need to solve. The _Update Every_ parameter controls after how many steps the model learns. Due to simplicity, it was selected to be 1 since there was no visible effect observed when increasing it and the model working quite well with the chosen parameter. For selecting the actions and in order to stabilize training, Ornstein-Uhlenbeck noise was implemented (_OUNoise_-class) and added to the agents _act_ function due to it having worked in previous implementations quite well.\n",
    "\n",
    "The exact value of the other hyperparameters were chosen based on experimentation, research and experience from the previous projects. For example, the TAU value was slightly increased to keep up with the larger networks (256&128 units instead of 128&128 ones). As for the learning rates, 0.001 have proved themselves as quite good LR for the chosen architecture as well as a GAMMA value of 0.98.\n",
    "\n",
    "\n",
    "\n",
    "The hyperparameters were chosen as follows:\n",
    "- Buffer size for the Replay Buffer: 500000\n",
    "- Batch size: 64\n",
    "- Discount factor GAMMA: 0.98\n",
    "- Parameter for soft update (TAU): 0.005\n",
    "- Learning rate actor: 0.001\n",
    "- Learning rate critic: 0.001\n",
    "- Weight decay: 0\n",
    "- Update parameter to determine after how many steps the model learns: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of results\n",
    "\n",
    "<img src=\\\"Plot of results_solution.png\\\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of last 100 episodes\n",
    "\n",
    "<img src=\\\"Plot of last 100 episodes.png\\\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodes needed\n",
    "\n",
    "The total number of episodes that was needed to solve the environment with an average score of +0.5 over 100 episodes was about 1000 in the submitted implementation. However, while we aimed for a score of +0.9 we noticed that like in the benchmark implementation, the scores started to drop at some point and only reached the average score of +0.9 after 1374 episodes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ideas for future work\n",
    "\n",
    "Developing the model to work in combination with Prioritized Experience Replay or Hindsight Experience Replay seems interesting as well as comparing a PPO approach to the DDPG one. However, one of the most interesting topics would be to stabilize the training in order to not letting the scores drop at some point. Suggestions on this would be very welcome. Some ideas could include letting the agents share one Replay Buffer and/or actor model as suggested in class.\n",
    "Also, it would be interesting to see how the trained agent would perform in similar but different environments to see if (1) it can learn faster in a new environment than a completely new agent and (2) there are other parallels between different environments and problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
